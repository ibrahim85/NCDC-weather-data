{
 "metadata": {
  "name": "",
  "signature": "sha256:1ed9dcb4ffee570476723bb46cc8b278871bc697fcaf1525bb7663bfeaf5ae74"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font face='\"Lucida Console\", Monaco, monospace' color = 'navy'>\n",
      "<style>a:hover {text-decoration: underline}</style>\n",
      "    NOAA\u2019s <a href = 'http://www.ncdc.noaa.gov/' style=\"color: black; text-decoration: none\" >National Climatic Data Center (NCDC)</a> maintains the world's largest climate data archive and provides <a href = 'http://www.ncdc.noaa.gov/data-access/quick-links' style=\"color: black; text-decoration: none\" >climate and weather datasets</a> to the users worldwide.  <br /> <br />\n",
      "For this project, we'll look at two of their datasets \n",
      "<ul><li> Global Historical Climatology Network-Daily  (GHCN-D) </li>\n",
      "<li> Global Summary of the Day  (GSOD) </li></ul> <br /> \n",
      "We'll analyze and compare the ditribution of the recordings temporally and spatially. \n",
      "</font> "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Importing all the required modules\n",
      "\n",
      "import os, sys, pickle, numpy, base64, zlib\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "home_dir='/home/ubuntu/UCSD_BigData'\n",
      "sys.path.append(home_dir+'/utils')\n",
      "\n",
      "# Importing some functions necessary for MRjobs\n",
      "from find_waiting_flow import *\n",
      "from AWS_keypair_management import *\n",
      "\n",
      "cur_dir  = home_dir+'/notebooks/weather_project' #working directory"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Loading required credentials and creating Config file for mrjob\n",
      "\n",
      "Creds= pickle.load(open('/home/ubuntu/Vault/Creds.pkl','rb'))\n",
      "pair=Creds['mrjob']\n",
      "key_id=pair['key_id']\n",
      "secret_key=pair['secret_key']\n",
      "ID=pair['ID']\n",
      "\n",
      "%cd $home_dir/utils/\n",
      "!python Make.mrjob.conf.py  #If EC2_VAULT is not defined, default location - '/home/ubuntu/Vault'\n",
      "%cd $home_dir/notebooks/weather_project"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/ubuntu/UCSD_BigData/utils\n",
        "Created the configuration file: /home/ubuntu/.mrjob.conf\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/ubuntu/UCSD_BigData/notebooks/weather_project\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font color = 'maroon'>1. Global Historical Climatology Network-Daily Dataset (GHCN-D) </font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font face='\"Lucida Console\", Monaco, monospace' color = 'navy'>\n",
      "The <a href = 'http://www1.ncdc.noaa.gov/pub/data/ghcn/daily/' style=\"color: black; text-decoration: none\" >GHCN-Daily dataset</a> contains daily climate records from over 80,000 stations worldwide. <br /> <br />\n",
      "\n",
      "The dataset includes five core readings :<br />\n",
      "1. Precipitation (PRCP)<br />\n",
      "2. Snowfall (SNOW) <br />\n",
      "3. Snow depth (SNWD) <br />\n",
      "4. Maximum temperature (TMAX)<br />\n",
      "5. Minimum temperature (TMIN)<br /> <br /> \n",
      "A typical single line in the data file has the following format<br />\n",
      "<b>Station ID, Measurement, Year, [readings for 365 days starting from January 1st (the last day of leap years is discarded)]</b>\n",
      "\n",
      "</font>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print a random line from the file\n",
      "\n",
      "!shuf -n1 ALL.csv_1024 "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "USC00126808,WT01,1953,1,1,1,,,,1,1,1,1,,,,,,,,,,1,1,1,,,,,,,,,,,,,1,,,,,,,1,1,,,,,,,,,,,,,,,,,,1,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,,,1,,,1,,,,,,1,,,,,,,,,,,,,,,,,1,1,,1,,,,,,,1,1,,,,1,1,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,,,,,,1,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,,,,,,,,,,\r\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Station Details"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font face='\"Lucida Console\", Monaco, monospace' color = 'navy'>\n",
      "Read Station Data into Pandas Dataframe\n",
      "</font>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#loading data corresponding to station IDs \n",
      "!gunzip stations.pkl.gz\n",
      "GHCN_Stations=pickle.load(open('stations.pkl', 'rb'))\n",
      "!gzip stations.pkl\n",
      "\n",
      "GHCN_Stations.head(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>latitude</th>\n",
        "      <th>longitude</th>\n",
        "      <th>elevation</th>\n",
        "      <th>state</th>\n",
        "      <th>name</th>\n",
        "      <th>GSNFLAG</th>\n",
        "      <th>HCNFLAG</th>\n",
        "      <th>WMOID</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>ACW00011604</th>\n",
        "      <td> 17.1167</td>\n",
        "      <td>-61.7833</td>\n",
        "      <td>   10.1</td>\n",
        "      <td> NaN</td>\n",
        "      <td> ST JOHNS COOLIDGE FLD</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>   NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>ACW00011647</th>\n",
        "      <td> 17.1333</td>\n",
        "      <td>-61.7833</td>\n",
        "      <td>   19.2</td>\n",
        "      <td> NaN</td>\n",
        "      <td>              ST JOHNS</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>   NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>AE000041196</th>\n",
        "      <td> 25.3330</td>\n",
        "      <td> 55.5170</td>\n",
        "      <td>   34.0</td>\n",
        "      <td> NaN</td>\n",
        "      <td>   SHARJAH INTER. AIRP</td>\n",
        "      <td> GSN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 41196</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>AF000040930</th>\n",
        "      <td> 35.3170</td>\n",
        "      <td> 69.0170</td>\n",
        "      <td> 3366.0</td>\n",
        "      <td> NaN</td>\n",
        "      <td>          NORTH-SALANG</td>\n",
        "      <td> GSN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 40930</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>AG000060390</th>\n",
        "      <td> 36.7167</td>\n",
        "      <td>  3.2500</td>\n",
        "      <td>   24.0</td>\n",
        "      <td> NaN</td>\n",
        "      <td>    ALGER-DAR EL BEIDA</td>\n",
        "      <td> GSN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 60390</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 8 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "             latitude  longitude  elevation state                   name  \\\n",
        "ACW00011604   17.1167   -61.7833       10.1   NaN  ST JOHNS COOLIDGE FLD   \n",
        "ACW00011647   17.1333   -61.7833       19.2   NaN               ST JOHNS   \n",
        "AE000041196   25.3330    55.5170       34.0   NaN    SHARJAH INTER. AIRP   \n",
        "AF000040930   35.3170    69.0170     3366.0   NaN           NORTH-SALANG   \n",
        "AG000060390   36.7167     3.2500       24.0   NaN     ALGER-DAR EL BEIDA   \n",
        "\n",
        "            GSNFLAG HCNFLAG  WMOID  \n",
        "ACW00011604     NaN     NaN    NaN  \n",
        "ACW00011647     NaN     NaN    NaN  \n",
        "AE000041196     GSN     NaN  41196  \n",
        "AF000040930     GSN     NaN  40930  \n",
        "AG000060390     GSN     NaN  60390  \n",
        "\n",
        "[5 rows x 8 columns]"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def station_details(stn):\n",
      "    stn_lat  = GHCN_Stations['latitude'][stn]\n",
      "    stn_lon  = GHCN_Stations['longitude'][stn]\n",
      "    stn_elvn = GHCN_Stations['elevation'][stn]\n",
      "    print \"\\nStation Details for '\"+ stn + \"'\"\n",
      "    print \"\\nName      : \", GHCN_Stations['name'][stn]\n",
      "    print \"Location  : \", \n",
      "    print abs(stn_lat), u\"\\u00b0\", ('S' if stn_lat < 0 else 'N'), ',' ,\n",
      "    print abs(stn_lon), u\"\\u00b0\", ('E' if stn_lat < 0 else 'W')\n",
      "    print \"Elevation : \", ('missing' if stn_elvn == -999.9 else str(stn_elvn) + ' m')\n",
      "    print \"Google maps link : https://www.google.com/maps/place/\"+str(stn_lat)+ \",\" + str(stn_lon) +'\\n'\n",
      "    \n",
      "station_details(\"USC00047741\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Station Details for 'USC00047741'\n",
        "\n",
        "Name      :  SAN DIEGO SEAWORLD\n",
        "Location  :  32.7672 \u00b0 N , 117.2258 \u00b0 W\n",
        "Elevation :  4.6 m\n",
        "Google maps link : https://www.google.com/maps/place/32.7672,-117.2258\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font color = 'maroon'>2. Global Summary of the Day Dataset (GSOD) </font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font face='\"Lucida Console\", Monaco, monospace' color = 'navy'>\n",
      "\n",
      "Daily weather elements in the <a href = 'http://www.ncdc.noaa.gov/cgi-bin/res40.pl?page=gsod.html' style=\"color: black; text-decoration: none\" >GSOD dataset</a> include : <br /> mean temperature, mean dew point, mean sea level pressure, station mean pressure, maximum and minimum temperature, maximum sustained wind speed and maximum gust, precipitation amount, snow depth, weather indicators etc. <br /> <br />\n",
      "\n",
      "A Typical single line in the data file has the following format - <br />\n",
      "<b>STN Number, WBAN Number, YEAR-MONTH-DATE, TEMP, Count, DEWP, Count, SLP, Count, STP, Count, VISIB, Count, WDSP, Count, MXSPD, GUST, TMAX(Flag), TMIN(Flag), PRCP(Flag), SNDP, FRSHTT </b><br /> <br />\n",
      "\n",
      "'Count' is used to indicate number of observations used in calculating mean. <br />\n",
      "'Flag' is used to indicate whether the mean was calculate using explicit measurement report or from 'hourly' data. <br />\n",
      "More details about the dataset contents can be found here - <a href = \"ftp://ftp.ncdc.noaa.gov/pub/data/gsod/readme.txt\" style=\"color: black; text-decoration: none\" >ftp://ftp.ncdc.noaa.gov/pub/data/gsod/readme.txt</a>\n",
      "\n",
      "</font>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print a random line from the file\n",
      "\n",
      "!shuf -n1 gsod.all.tsv_1024"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "150100,99999,19890809,67.1,17,64.0,12,1010.1,8,995.7,8,4.4,17,3.5,15,14.0,999.9,77.4,60.8*,0.80F,999.9,110010\r\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font color = 'maroon'>Collecting Statistics using Map Reduce</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font face='\"Lucida Console\", Monaco, monospace' color = 'navy'>\n",
      "For both the datasets, we collect the count of readings defined out of 365 possible days for each (measurement, year) pair whenever available.\n",
      "</font>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile collect_GHCNStats.py\n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "collect the statistics for each station.\n",
      "\"\"\"\n",
      "import re,pickle,base64,zlib\n",
      "from sys import stderr\n",
      "import sys\n",
      "\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages') # a hack because anaconda made mrjob unreachable\n",
      "from mrjob.job import MRJob\n",
      "from mrjob.protocol import *\n",
      "\n",
      "import traceback\n",
      "from functools import wraps\n",
      "from sys import stderr\n",
      "\n",
      "\"\"\"this decorator is intended for decorating a function, not a\n",
      "generator.  Therefore to use it in the context of mrjob, the generator\n",
      "should call a function that handles a single input records, and that\n",
      "function should be decorated.\n",
      "\n",
      "The reason is that if a generator throws an exception it exits and\n",
      "cannot process any more records.\n",
      "\n",
      "\"\"\"\n",
      "def ECatch(func):\n",
      "    f_name=func.__name__\n",
      "    #@wraps(func) cd\n",
      "    def inner(self,*args,**kwargs):\n",
      "        try:\n",
      "            self.increment_counter(self.__class__.__name__,'total in '+f_name,1)\n",
      "            return func(self,*args,**kwargs)\n",
      "        except Exception as e:\n",
      "            self.increment_counter(self.__class__.__name__,'errors in '+f_name,1)\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e))\n",
      "            traceback.print_exc(file=stderr)\n",
      "            stderr.write('Arguments were %s, %s\\n'%(args,kwargs))\n",
      "            pass\n",
      "    return inner        \n",
      "\n",
      "\"\"\"\n",
      "Functions for encoding and decoding arbitrary object into ascii \n",
      "so that they can be passed through the hadoop streaming interface.\n",
      "\"\"\"\n",
      "def loads(eVal):\n",
      "    \"\"\" Decode a string into a value \"\"\"\n",
      "    return pickle.loads(zlib.decompress(base64.b64decode(eVal)))\n",
      "\n",
      "def dumps(Value):\n",
      "    \"\"\" Encode a value as a string \"\"\"\n",
      "    return base64.b64encode(zlib.compress(pickle.dumps(Value),9))\n",
      "\n",
      "\n",
      "class MRWeather(MRJob):\n",
      "\n",
      "    @ECatch\n",
      "    def map_one(self,line):\n",
      "        return line.split(',')\n",
      "    \n",
      "    def mapper(self, _, line):\n",
      "        elements=self.map_one(line)\n",
      "        if(elements[1]!='year'):\n",
      "            yield(elements[0],elements[1:])\n",
      "            \n",
      "    def check_integrity(self,meas,year,length):\n",
      "        if year<1000 or year > 2014: return False\n",
      "        if meas=='': return False\n",
      "        if length != 367: return False\n",
      "        return True\n",
      "    \n",
      "    @ECatch\n",
      "    def reduce_one(self,S,vector):\n",
      "        meas=vector[0]\n",
      "        year=int(vector[1])\n",
      "        length=len(vector)\n",
      "        number_defined=sum([e!='' for e in vector[2:]])\n",
      "        assert self.check_integrity(meas,year,length)==True\n",
      "        S[(meas,year)]=number_defined\n",
      "        \n",
      "    def reducer(self, station, vectors):\n",
      "        S={}\n",
      "        for vector in vectors:\n",
      "            self.reduce_one(S,vector)\n",
      "        yield(station,dumps(S))\n",
      "                              \n",
      "if __name__ == '__main__':\n",
      "    MRWeather.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting collect_GHCNStats.py\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python collect_GHCNStats.py ALL.csv_1024 > ghcn_counts_local"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using configs in /home/ubuntu/.mrjob.conf\r\n",
        "creating tmp directory /tmp/collect_GHCNStats.ubuntu.20140607.014639.485952\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "writing to /tmp/collect_GHCNStats.ubuntu.20140607.014639.485952/step-0-mapper_part-00000\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counters from step 1:\r\n",
        "  MRWeather:\r\n",
        "    total in map_one: 9139\r\n",
        "writing to /tmp/collect_GHCNStats.ubuntu.20140607.014639.485952/step-0-mapper-sorted\r\n",
        "> sort /tmp/collect_GHCNStats.ubuntu.20140607.014639.485952/step-0-mapper_part-00000\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "writing to /tmp/collect_GHCNStats.ubuntu.20140607.014639.485952/step-0-reducer_part-00000\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counters from step 1:\r\n",
        "  MRWeather:\r\n",
        "    total in map_one: 9139\r\n",
        "    total in reduce_one: 9138\r\n",
        "Moving /tmp/collect_GHCNStats.ubuntu.20140607.014639.485952/step-0-reducer_part-00000 -> /tmp/collect_GHCNStats.ubuntu.20140607.014639.485952/output/part-00000\r\n",
        "Streaming final output from /tmp/collect_GHCNStats.ubuntu.20140607.014639.485952/output\r\n",
        "removing tmp directory /tmp/collect_GHCNStats.ubuntu.20140607.014639.485952\r\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job_flow_id=find_waiting_flow(key_id,secret_key)\n",
      "!python collect_GHCNStats.py -r emr --emr-job-flow-id $job_flow_id s3://vineel-bucket/ALL.csv > ghcn_counts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<boto.emr.emrobject.JobFlow object at 0x4e25d10> no_script.yoavfreund.20140605.015826.598790 j-2SIBPG1O1P2Q0 WAITING\n",
        "<boto.emr.emrobject.JobFlow object at 0x6635c10> no_script.yoavfreund.20140605.052302.702240 j-IAZAAX9EZ3LP WAITING\n",
        "<boto.emr.emrobject.JobFlow object at 0x4e1a110> no_script.yoavfreund.20140605.052356.755021 j-16A0QA0I8H218 WAITING\n",
        "<boto.emr.emrobject.JobFlow object at 0x4e27210> no_script.yoavfreund.20140605.052406.630684 j-2MX6SRUZWX3O WAITING\n",
        "<boto.emr.emrobject.JobFlow object at 0x4e303d0> no_script.yoavfreund.20140607.003028.790806 j-K45WHE9UDB72 WAITING\n",
        "<boto.emr.emrobject.JobFlow object at 0x4e30650> no_script.yoavfreund.20140607.003037.202650 j-K1J1HPE1883F WAITING\n",
        "<boto.emr.emrobject.JobFlow object at 0x4e308d0> no_script.yoavfreund.20140607.003045.492952 j-2EYI61B8ITNL WAITING\n",
        "<boto.emr.emrobject.JobFlow object at 0x4e30b50> no_script.yoavfreund.20140607.003053.752693 j-3L99L6PVQ9W0E WAITING\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using configs in /home/ubuntu/.mrjob.conf\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "creating new scratch bucket mrjob-27ee2b9168f3192a\r\n",
        "using s3://mrjob-27ee2b9168f3192a/tmp/ as our scratch dir on S3\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "creating tmp directory /tmp/collect_GHCNStats.ubuntu.20140607.014936.170558\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "creating S3 bucket 'mrjob-27ee2b9168f3192a' to use as scratch space\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Copying non-input files into s3://mrjob-27ee2b9168f3192a/tmp/collect_GHCNStats.ubuntu.20140607.014936.170558/files/\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Adding our job to existing job flow j-3L99L6PVQ9W0E\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 30.1s ago, status RUNNING: Running step (collect_GHCNStats.ubuntu.20140607.014936.170558: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 60.3s ago, status RUNNING: Running step (collect_GHCNStats.ubuntu.20140607.014936.170558: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 90.5s ago, status RUNNING: Running step (collect_GHCNStats.ubuntu.20140607.014936.170558: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 120.7s ago, status RUNNING: Running step (collect_GHCNStats.ubuntu.20140607.014936.170558: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 150.8s ago, status RUNNING: Running step (collect_GHCNStats.ubuntu.20140607.014936.170558: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 181.0s ago, status RUNNING: Running step (collect_GHCNStats.ubuntu.20140607.014936.170558: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 211.3s ago, status RUNNING: Running step (collect_GHCNStats.ubuntu.20140607.014936.170558: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 241.4s ago, status RUNNING: Running step (collect_GHCNStats.ubuntu.20140607.014936.170558: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 271.6s ago, status RUNNING: Running step (collect_GHCNStats.ubuntu.20140607.014936.170558: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 301.8s ago, status RUNNING: Running step (collect_GHCNStats.ubuntu.20140607.014936.170558: Step 1 of 1)\r\n"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile collect_GSODStats.py\n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "collect the statistics for each station.\n",
      "\"\"\"\n",
      "import re,pickle,base64,zlib\n",
      "from sys import stderr\n",
      "import sys\n",
      "\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages') # a hack because anaconda made mrjob unreachable\n",
      "from mrjob.job import MRJob\n",
      "from mrjob.protocol import *\n",
      "\n",
      "import traceback\n",
      "from functools import wraps\n",
      "from sys import stderr\n",
      "\n",
      "\"\"\"this decorator is intended for decorating a function, not a\n",
      "generator.  Therefore to use it in the context of mrjob, the generator\n",
      "should call a function that handles a single input records, and that\n",
      "function should be decorated.\n",
      "\n",
      "The reason is that if a generator throws an exception it exits and\n",
      "cannot process any more records.\n",
      "\n",
      "\"\"\"\n",
      "def ECatch(func):\n",
      "    f_name=func.__name__\n",
      "    #@wraps(func) cd\n",
      "    def inner(self,*args,**kwargs):\n",
      "        try:\n",
      "            self.increment_counter(self.__class__.__name__,'total in '+f_name,1)\n",
      "            return func(self,*args,**kwargs)\n",
      "        except Exception as e:\n",
      "            self.increment_counter(self.__class__.__name__,'errors in '+f_name,1)\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e))\n",
      "            traceback.print_exc(file=stderr)\n",
      "            stderr.write('Arguments were %s, %s\\n'%(args,kwargs))\n",
      "            pass\n",
      "    return inner        \n",
      "\n",
      "\"\"\"\n",
      "Functions for encoding and decoding arbitrary object into ascii \n",
      "so that they can be passed through the hadoop streaming interface.\n",
      "\"\"\"\n",
      "def loads(eVal):\n",
      "    \"\"\" Decode a string into a value \"\"\"\n",
      "    return pickle.loads(zlib.decompress(base64.b64decode(eVal)))\n",
      "\n",
      "def dumps(Value):\n",
      "    \"\"\" Encode a value as a string \"\"\"\n",
      "    return base64.b64encode(zlib.compress(pickle.dumps(Value),9))\n",
      "\n",
      "\n",
      "class MRWeather(MRJob):\n",
      "\n",
      "    @ECatch\n",
      "    def map_one(self,line):\n",
      "        return line.split(',')\n",
      "    \n",
      "    def mapper(self, _, line):\n",
      "        elements=self.map_one(line)\n",
      "        if(elements[1]!='year'):\n",
      "            yield((elements[0], elements[1]/10000),[elements[1]%1000,elements[-4:-1]])\n",
      "            \n",
      "    def check_integrity(self,meas,year,length):\n",
      "        if year<1000 or year > 2014: return False\n",
      "        if meas=='': return False\n",
      "        if length != 367: return False\n",
      "        return True\n",
      "    \n",
      "    @ECatch\n",
      "    def reduce_one(self,S,vector):\n",
      "        meas=vector[0]\n",
      "        year=int(vector[1])\n",
      "        length=len(vector)\n",
      "        number_defined=sum([e!='' for e in vector[2:]])\n",
      "        assert self.check_integrity(meas,year,length)==True\n",
      "        S[(meas,year)]=number_defined\n",
      "        \n",
      "    def reducer(self, station, vectors):\n",
      "        S={}\n",
      "        yield(station, None)\n",
      "                              \n",
      "if __name__ == '__main__':\n",
      "    MRWeather.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}