{
 "metadata": {
  "name": "",
  "signature": "sha256:5000b7688830ba10dd3a0bbec8affdfdb97d189721b71ea52953cdf26714b581"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font face='\"Lucida Console\", Monaco, monospace' color = 'navy'>\n",
      "<style>a:hover {text-decoration: underline}</style>\n",
      "    NOAA\u2019s <a href = 'http://www.ncdc.noaa.gov/' style=\"color: black; text-decoration: none\" >National Climatic Data Center (NCDC)</a> maintains the world's largest climate data archive and provides <a href = 'http://www.ncdc.noaa.gov/data-access/quick-links' style=\"color: black; text-decoration: none\" >climate and weather datasets</a> to the users worldwide.  <br /> <br />\n",
      "For this project, we'll look at two of their datasets \n",
      "<ul><li> Global Historical Climatology Network-Daily  (GHCN-D) </li>\n",
      "<li> Global Summary of the Day  (GSOD) </li></ul> <br /> \n",
      "We'll analyze and compare the ditribution of the recordings temporally and spatially. \n",
      "</font> "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Importing all the required modules\n",
      "\n",
      "import os, sys, numpy, base64, zlib, pickle\n",
      "import pandas as pd\n",
      "import cPickle as cpkl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "home_dir='/home/ubuntu/UCSD_BigData'\n",
      "sys.path.append(home_dir+'/utils')\n",
      "\n",
      "# Importing some functions necessary for MRjobs\n",
      "from find_waiting_flow import *\n",
      "from AWS_keypair_management import *\n",
      "\n",
      "cur_dir  = home_dir+'/notebooks/weather_project' #working directory"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Loading required credentials and creating Config file for mrjob\n",
      "\n",
      "Creds= pickle.load(open('/home/ubuntu/Vault/Creds.pkl','rb'))\n",
      "pair=Creds['mrjob']\n",
      "key_id=pair['key_id']\n",
      "secret_key=pair['secret_key']\n",
      "ID=pair['ID']\n",
      "\n",
      "%cd $home_dir/utils/\n",
      "!python Make.mrjob.conf.py  #If EC2_VAULT is not defined, default location - '/home/ubuntu/Vault'\n",
      "%cd $home_dir/notebooks/weather_project"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/ubuntu/UCSD_BigData/utils\n",
        "Created the configuration file: /home/ubuntu/.mrjob.conf\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/ubuntu/UCSD_BigData/notebooks/weather_project\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font face='\"Lucida Console\", Monaco, monospace' color = 'navy'>\n",
      "Importing functions  </font>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from functions_weather import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile functions_weather.py\n",
      "import os, sys, numpy, base64, zlib, pickle\n",
      "import pandas as pd\n",
      "\n",
      "def loads(eVal):\n",
      "    \"\"\" Decode a string into a value \"\"\"\n",
      "    return pickle.loads(zlib.decompress(base64.b64decode(eVal)))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting functions_weather.py\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font color = 'maroon'>1. Global Historical Climatology Network-Daily Dataset (GHCN-D) </font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font face='\"Lucida Console\", Monaco, monospace' color = 'navy'>\n",
      "The <a href = 'http://www1.ncdc.noaa.gov/pub/data/ghcn/daily/' style=\"color: black; text-decoration: none\" >GHCN-Daily dataset</a> contains daily climate records from over 80,000 stations worldwide. <br /> <br />\n",
      "\n",
      "The dataset includes five core readings :<br />\n",
      "1. Precipitation (PRCP)<br />\n",
      "2. Snowfall (SNOW) <br />\n",
      "3. Snow depth (SNWD) <br />\n",
      "4. Maximum temperature (TMAX)<br />\n",
      "5. Minimum temperature (TMIN)<br /> <br /> \n",
      "A typical single line in the data file has the following format<br />\n",
      "<b>Station ID, Measurement, Year, [readings for 365 days starting from January 1st (the last day of leap years is discarded)]</b>\n",
      "\n",
      "</font>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print a random line from the file\n",
      "\n",
      "!shuf -n1 ALL.csv_1024 "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CA007057567,TMIN,1985,-180,-180,-210,-140,-100,-195,-270,-235,-210,-180,-185,-175,-180,-140,-140,-230,-230,-270,-245,-150,-230,-240,-130,-140,-200,-195,-195,-220,-185,-195,-270,-125,-145,-205,-230,-260,-235,-270,-205,-120,-110,-105,-150,-30,-20,-70,-110,-150,-100,-190,-125,-270,-70,-5,-10,-50,-50,-75,-200,-105,-15,-180,-250,-220,-205,-200,-110,-130,-125,-30,0,0,-5,-60,-150,-125,-150,-210,,-150,-160,-85,-75,-105,-110,-105,-25,-20,-25,-135,-60,-35,-30,-15,-45,-5,0,-55,-100,-135,-30,-100,-100,-95,-20,15,-100,-95,0,-50,-45,-10,-30,-20,-50,65,-10,-5,30,75,50,-20,10,-15,-35,-45,25,-10,-65,80,0,50,40,25,-10,5,120,60,60,20,75,80,5,5,110,25,60,65,-15,35,100,70,95,100,10,35,90,25,60,45,125,55,75,90,85,90,80,90,100,100,130,80,70,105,150,,95,75,120,105,110,125,145,150,140,140,160,145,130,110,135,135,60,80,105,185,170,130,120,150,150,145,150,95,70,110,180,110,70,110,115,90,85,70,90,125,120,120,125,190,120,110,140,130,60,110,130,140,85,35,110,130,130,95,70,85,110,140,145,140,40,70,60,15,110,20,100,130,120,110,125,35,75,20,10,5,10,50,65,40,70,130,165,175,40,70,100,100,15,90,105,65,105,85,70,10,-10,95,50,30,-15,80,70,-5,-30,-10,30,35,60,-5,-20,95,30,-50,-25,-15,-10,65,20,30,10,-35,-20,-75,-75,45,30,-10,25,40,30,10,-30,-10,-60,-95,-30,-120,-110,-170,-120,5,-30,0,-10,-80,-50,-50,-85,-175,-120,-130,-195,-135,-75,-30,-105,-155,-135,-100,-140,-80,-150,-130,-80,-100,-105,-130,-250,-180,-150,-180,-290,-270,-230,-230,-170,-90,-150,-270,-300,-170,-120,-170,-220\r\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Station Details"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font face='\"Lucida Console\", Monaco, monospace' color = 'navy'>\n",
      "Read Station Data into Pandas Dataframe\n",
      "</font>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#loading data corresponding to station IDs \n",
      "!gunzip stations.pkl.gz\n",
      "GHCN_Stations=pickle.load(open('stations.pkl', 'rb'))\n",
      "!gzip stations.pkl\n",
      "\n",
      "GHCN_Stations.head(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>latitude</th>\n",
        "      <th>longitude</th>\n",
        "      <th>elevation</th>\n",
        "      <th>state</th>\n",
        "      <th>name</th>\n",
        "      <th>GSNFLAG</th>\n",
        "      <th>HCNFLAG</th>\n",
        "      <th>WMOID</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>ACW00011604</th>\n",
        "      <td> 17.1167</td>\n",
        "      <td>-61.7833</td>\n",
        "      <td>   10.1</td>\n",
        "      <td> NaN</td>\n",
        "      <td> ST JOHNS COOLIDGE FLD</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>   NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>ACW00011647</th>\n",
        "      <td> 17.1333</td>\n",
        "      <td>-61.7833</td>\n",
        "      <td>   19.2</td>\n",
        "      <td> NaN</td>\n",
        "      <td>              ST JOHNS</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>   NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>AE000041196</th>\n",
        "      <td> 25.3330</td>\n",
        "      <td> 55.5170</td>\n",
        "      <td>   34.0</td>\n",
        "      <td> NaN</td>\n",
        "      <td>   SHARJAH INTER. AIRP</td>\n",
        "      <td> GSN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 41196</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>AF000040930</th>\n",
        "      <td> 35.3170</td>\n",
        "      <td> 69.0170</td>\n",
        "      <td> 3366.0</td>\n",
        "      <td> NaN</td>\n",
        "      <td>          NORTH-SALANG</td>\n",
        "      <td> GSN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 40930</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>AG000060390</th>\n",
        "      <td> 36.7167</td>\n",
        "      <td>  3.2500</td>\n",
        "      <td>   24.0</td>\n",
        "      <td> NaN</td>\n",
        "      <td>    ALGER-DAR EL BEIDA</td>\n",
        "      <td> GSN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 60390</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 8 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "             latitude  longitude  elevation state                   name  \\\n",
        "ACW00011604   17.1167   -61.7833       10.1   NaN  ST JOHNS COOLIDGE FLD   \n",
        "ACW00011647   17.1333   -61.7833       19.2   NaN               ST JOHNS   \n",
        "AE000041196   25.3330    55.5170       34.0   NaN    SHARJAH INTER. AIRP   \n",
        "AF000040930   35.3170    69.0170     3366.0   NaN           NORTH-SALANG   \n",
        "AG000060390   36.7167     3.2500       24.0   NaN     ALGER-DAR EL BEIDA   \n",
        "\n",
        "            GSNFLAG HCNFLAG  WMOID  \n",
        "ACW00011604     NaN     NaN    NaN  \n",
        "ACW00011647     NaN     NaN    NaN  \n",
        "AE000041196     GSN     NaN  41196  \n",
        "AF000040930     GSN     NaN  40930  \n",
        "AG000060390     GSN     NaN  60390  \n",
        "\n",
        "[5 rows x 8 columns]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ghcn_station_details(stn):\n",
      "    stn_lat  = GHCN_Stations['latitude'][stn]\n",
      "    stn_lon  = GHCN_Stations['longitude'][stn]\n",
      "    stn_elvn = GHCN_Stations['elevation'][stn]\n",
      "    print \"\\nStation Details for '\"+ stn + \"'\"\n",
      "    print \"\\nName      : \", GHCN_Stations['name'][stn]\n",
      "    print \"Location  : \", \n",
      "    print abs(stn_lat), u\"\\u00b0\", ('S' if stn_lat < 0 else 'N'), ',' ,\n",
      "    print abs(stn_lon), u\"\\u00b0\", ('E' if stn_lat < 0 else 'W')\n",
      "    print \"Elevation : \", ('missing' if stn_elvn == -999.9 else str(stn_elvn) + ' m')\n",
      "    print \"Google maps link : https://www.google.com/maps/place/\"+str(stn_lat)+ \",\" + str(stn_lon) +'\\n'\n",
      "    \n",
      "ghcn_station_details(\"USC00047741\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Station Details for 'USC00047741'\n",
        "\n",
        "Name      :  SAN DIEGO SEAWORLD\n",
        "Location  :  32.7672 \u00b0 N , 117.2258 \u00b0 W\n",
        "Elevation :  4.6 m\n",
        "Google maps link : https://www.google.com/maps/place/32.7672,-117.2258\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font color = 'maroon'>2. Global Summary of the Day Dataset (GSOD) </font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font face='\"Lucida Console\", Monaco, monospace' color = 'navy'>\n",
      "\n",
      "Daily weather elements in the <a href = 'http://www.ncdc.noaa.gov/cgi-bin/res40.pl?page=gsod.html' style=\"color: black; text-decoration: none\" >GSOD dataset</a> include : <br /> mean temperature, mean dew point, mean sea level pressure, station mean pressure, maximum and minimum temperature, maximum sustained wind speed and maximum gust, precipitation amount, snow depth, weather indicators etc. <br /> <br />\n",
      "\n",
      "A Typical single line in the data file has the following format - <br />\n",
      "<b>STN Number, WBAN Number, YEAR-MONTH-DATE, TEMP, Count, DEWP, Count, SLP, Count, STP, Count, VISIB, Count, WDSP, Count, MXSPD, GUST, TMAX(Flag), TMIN(Flag), PRCP(Flag), SNDP, FRSHTT </b><br /> <br />\n",
      "\n",
      "'Count' is used to indicate number of observations used in calculating mean. <br />\n",
      "'Flag' is used to indicate whether the mean was calculate using explicit measurement report or from 'hourly' data. <br />\n",
      "More details about the dataset contents can be found here - <a href = \"ftp://ftp.ncdc.noaa.gov/pub/data/gsod/readme.txt\" style=\"color: black; text-decoration: none\" >ftp://ftp.ncdc.noaa.gov/pub/data/gsod/readme.txt</a>\n",
      "\n",
      "</font>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print a random line from the file\n",
      "\n",
      "!shuf -n1 gsod.all.tsv_1024"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "949750,99999,19890818,53.4,9,40.3,9,1020.6,9,1020.2,9,31.1,9,10.3,9,15.0,999.9,59.4,44.1,0.00D,999.9,000000\r\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font face='\"Lucida Console\", Monaco, monospace' color = 'navy'>\n",
      "Read Station Data into Pandas Dataframe\n",
      "</font>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#loading data corresponding to station IDs \n",
      "GSOD_Stations=pd.read_csv(\"gsod_stations_cleaned.csv\", sep='\\t', converters={'USAF': str, 'WBAN': str})\n",
      "GSOD_Stations.head(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>USAF</th>\n",
        "      <th>WBAN</th>\n",
        "      <th>STATION NAME</th>\n",
        "      <th>CTRY</th>\n",
        "      <th> ST</th>\n",
        "      <th>CALL</th>\n",
        "      <th>LAT</th>\n",
        "      <th>LON</th>\n",
        "      <th>ELEV(.1M)</th>\n",
        "      <th>BEGIN</th>\n",
        "      <th>END</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 006852</td>\n",
        "      <td> 99999</td>\n",
        "      <td>       SENT</td>\n",
        "      <td>  SW</td>\n",
        "      <td>  SZ</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 46817</td>\n",
        "      <td>  10350</td>\n",
        "      <td> 14200</td>\n",
        "      <td>  NO DATA</td>\n",
        "      <td>  NO DATA</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 007005</td>\n",
        "      <td> 99999</td>\n",
        "      <td> CWOS 07005</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>-99999</td>\n",
        "      <td>-999999</td>\n",
        "      <td>-99999</td>\n",
        "      <td> 20120127</td>\n",
        "      <td> 20120127</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 007010</td>\n",
        "      <td> 99999</td>\n",
        "      <td> CWOS 07010</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>-99999</td>\n",
        "      <td>-999999</td>\n",
        "      <td>-99999</td>\n",
        "      <td>  NO DATA</td>\n",
        "      <td>  NO DATA</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 007011</td>\n",
        "      <td> 99999</td>\n",
        "      <td> CWOS 07011</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>-99999</td>\n",
        "      <td>-999999</td>\n",
        "      <td>-99999</td>\n",
        "      <td> 20111025</td>\n",
        "      <td> 20121129</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 007012</td>\n",
        "      <td> 99999</td>\n",
        "      <td> CWOS 07012</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>-99999</td>\n",
        "      <td>-999999</td>\n",
        "      <td>-99999</td>\n",
        "      <td>  NO DATA</td>\n",
        "      <td>  NO DATA</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 11 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "     USAF   WBAN STATION NAME CTRY   ST CALL    LAT     LON  ELEV(.1M)  \\\n",
        "0  006852  99999         SENT   SW   SZ  NaN  46817   10350      14200   \n",
        "1  007005  99999   CWOS 07005  NaN  NaN  NaN -99999 -999999     -99999   \n",
        "2  007010  99999   CWOS 07010  NaN  NaN  NaN -99999 -999999     -99999   \n",
        "3  007011  99999   CWOS 07011  NaN  NaN  NaN -99999 -999999     -99999   \n",
        "4  007012  99999   CWOS 07012  NaN  NaN  NaN -99999 -999999     -99999   \n",
        "\n",
        "      BEGIN       END  \n",
        "0   NO DATA   NO DATA  \n",
        "1  20120127  20120127  \n",
        "2   NO DATA   NO DATA  \n",
        "3  20111025  20121129  \n",
        "4   NO DATA   NO DATA  \n",
        "\n",
        "[5 rows x 11 columns]"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font color = 'maroon'>3. Collecting Statistics using Map Reduce</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font face='\"Lucida Console\", Monaco, monospace' color = 'navy'>\n",
      "For both the datasets, we collect the count of readings defined out of 365 possible days for each (measurement, year) pair whenever available.\n",
      "</font>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!python collect_GHCNStats.py ALL.csv_1024 > ghcn_counts_local"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#job_flow_id=find_waiting_flow(key_id,secret_key)\n",
      "#!python collect_GHCNStats.py -r emr --emr-job-flow-id $job_flow_id s3://vineel-bucket/ALL.csv > ghcn_counts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!python collect_GSODStats.py gsod.all.tsv_1024 > gsod_counts_local"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#job_flow_id=find_waiting_flow(key_id,secret_key)\n",
      "#!python collect_GSODStats.py -r emr --emr-job-flow-id $job_flow_id s3://vineel-bucket/gsod.all.tsv > gsod_counts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_countsdata_to_dict(loc):\n",
      "    stn_data = {}\n",
      "    f = open(loc)\n",
      "    for line in f:\n",
      "        stn, data_enc = line.split()\n",
      "        stn_data[stn[1:-1]] = loads(data_enc[1:-1])\n",
      "    return stn_data\n",
      "\n",
      "ghcn_counts = load_countsdata_to_dict('ghcn_counts')\n",
      "gsod_counts = load_countsdata_to_dict('gsod_counts')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ghcn_mat_counts "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font color = 'maroon'>4. Analysis of datasets</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font face='\"Lucida Console\", Monaco, monospace' color = 'navy'> \n",
      "Quick check to find the range of years for which readings are available </font>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_min_max_years(counts):\n",
      "    min_year, max_year = 2014, 1000 #initialize with obvious wrong values\n",
      "    for stn in counts.keys():\n",
      "        for (yr,_) in counts[stn].keys():\n",
      "            if(yr < min_year): min_year = yr\n",
      "            if(yr > max_year): max_year = yr\n",
      "    return (min_year, max_year)\n",
      "\n",
      "ghcn_min_year, ghcn_max_year = find_min_max_years(ghcn_counts)\n",
      "gsod_min_year, gsod_max_year = find_min_max_years(gsod_counts)\n",
      "\n",
      "print 'Readings avaialble for GHCN-D between the years:', ghcn_min_year, '-', ghcn_max_year\n",
      "print 'Readings avaialble for GSOD between the years:', gsod_min_year, '-', gsod_max_year"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Temporal Evolution of Station Networks"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}